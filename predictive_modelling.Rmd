---
title: "Spotify - Music of the Future"
output: html_notebook
author: Rachel Liu, Bohan Jiang, Dapo Folami, Justin Li
team: Team 4 - CS Duals
date: December 9th, 2020
---

##### *Note that it may take a significant amount of time and processing power to run this entire notebook, due to the number of models trained.* 

## **Project Value and Impact**
The overarching objective for this project is to predict the next hit songs and understand what features drives popularity over time. This will give music professionals (music producer, song writer, singer, etc) a data-driven approach to be successful in the industry.
In order to do this, we will be analyzing different variable relations, music trends over different decades, and feature significance with popularity. In the next few sections, we have shown in details different steps to clean the dataset, draw different insights/visualization based on initial analysis of the data, and create two predictive models to meet our objective. 

## **Data Set Description**
For this project, our team decided to take a data-driven approach to evaluate music. We are using the Spotify Dataset from 1921 to 2020 that consists of over 160,000 different tracks (from Kaggle). In order to stay consistent in the evaluation, all data was sourced from the Spotify Web API. The following set of data is combination of Primary, Numerical, Dummy(binary), and Categorical data. This allows us to explore different types of models and draw different insights. Here are all the variables that is included in the data set:

**Primary Key** \n

- id: this an unique key comprised of numbers and characters that is assigned to each track generated by Spotify \n


**Numerical** \n

- acousticness: range from 0(LOW) to 1(HIGH) \n

- danceability: range from 0(LOW) to 1(HIGH) \n

- energy: range from 0(LOW) to 1(HIGH) \n

- duration_ms: majority range from 200,000 to 300,000 \n

- instrumentalness: range from 0(LOW) to 1(HIGH) \n

- valence: range from 0(LOW) to 1(HIGH) \n

- popularity: range from 0(LOW) to 100(HIGH). The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. This is the main dependent variable. \n

- tempo: majority range from 50(LOW) to 150(HIGH) \n

- liveness: range from 0(LOW) to 1(HIGH) \n

- loudness majority range from -60 to 0 \n

- speechiness: range from 0(LOW) to 1(HIGH) \n

- year: range from 1921 to 2020 \n


**Dummy/Binary** \n

- mode: 0 represents minor and 1 represents major \n

- explicit: 0 represents no explicit content and 1 represents explicit content \n

    
**Categorical** \n

- key: this consists of all different music keys on octave encoded from 0 to 11 (i.e. C = 0, C# = 1, etc...) \n

- artists: the artist of the track \n

- release_date: the date of release in yyyy-mm-dd format \n

- name: the name of the track \n

<br />

#### Further documentation for a specific description for each of the audio features can be found [here](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/). \n \n

Install all necessary packages before running this notebook. 
```{r}
# Install all necessary packages
pkgs = c("ggplot2", "dplyr", "plotly","hrbrthemes","forecast","xts","Metrics","psych","dygraphs","GGally","tidyverse","tidyquant","cranlogs","corrr","cowplot","gplots","plm","lubridate","fastDummies","randomForest","randomForest","doParallel","caret","neuralnet","xgboost","h2o","readr","plyr","formattable","RWeka","qdap","tm","magrittr","corrplot","treemap","viridisLite","factoextra","gridExtra","kableExtra", "smooth", "nnfor")
install.packages(pkgs)
```

Import all required libraries to run this notebook.
```{r}
# Libraries
options(warn=-1)
suppressPackageStartupMessages({
library(ggplot2)
library(dplyr)
library(plotly)
library(hrbrthemes)
library(forecast)
library(xts)
library(Metrics)
library(psych)
library(dygraphs)
library(GGally)
library(tidyverse)
library(tidyquant)  
library(cranlogs)   
library(corrr)      
library(cowplot)   
library(gplots)
library(plm)
library(lubridate)
library(fastDummies)
library(randomForest)
library(doParallel) 
library(caret)
library(neuralnet)
library(xgboost)
library(h2o)
library(readr)
library(plyr)
library(formattable)
library(RWeka)
library(qdap)
library(tm)
library(magrittr)
library(corrplot)
library(treemap)
library(viridisLite)
library(factoextra)
library(gridExtra)
library(kableExtra)
library(smooth)
library(nnfor)
})
```

Change display options when printing matrices. 
```{r}
options(repr.matrix.max.rows=100, repr.matrix.max.cols=20)
```
<br />

## **Data Import, Summary and Cleaning**


#### Import all datasets into dataframes.
```{r}
#Import datasets into DFs

# This is the main dataset that contains all unique songs 
df = read.csv(file='./data/data.csv')

# Dataset that groups songs by genres
df_genre = read.csv(file='./data/data_by_genres.csv')

# Dataset that groups songs by artist
df_artist = read.csv(file='./data/data_by_artist.csv')

# Dataset that groups songs by year
df_year = read.csv(file='./data/data_by_year.csv')

# Dataset that group songs by artist and genre 
df_genres2 = read.csv(file='./data/data_w_genres.csv')
```


```{r}
# Preview all DFs
head(df)
head(df_genre)
head(df_artist)
head(df_year)
head(df_genres2)
```
<br />

#### Data Cleaning
```{r}
# Take only unique rows - remove duplicates
df <- df %>% distinct()
df_genre <- df_genre %>% distinct()
df_artist <- df_artist %>% distinct()
df_year <- df_year %>% distinct()
df_genres2 <- df_genres2 %>% distinct()

# Remove null rows in R
df <- df[rowSums(is.na(df)) == 0,]
df_genre <- df_genre[rowSums(is.na(df_genre)) == 0,]
df_artist <- df_artist[rowSums(is.na(df_artist)) == 0,]
df_year <- df_year[rowSums(is.na(df_year)) == 0,]
df_genres2 <- df_genres2[rowSums(is.na(df_genres2)) == 0,]

# Change year column to a double value for df and df_year
df$year <- as.Date(as.character(df$year), format = "%Y")
df$year = lubridate::year(df$year)

df_year$year <- as.Date(as.character(df_year$year), format = "%Y")
df_year$year = lubridate::year(df_year$year)
head(df)
head(df_year)
```

<br />

#### Data Summary of all Datasets
```{r}
# Get summary of main dataframe

# Data Dimensions, Column names, column types, preview of data
glimpse(df)
# Statistical summary of every column
summary(df)
# Print all column names
colnames(df)
```
The above dataset is an unbalanced dataset with differing quantities of cross-sectional observations of new songs every year, from 1921 - 2020. 

```{r}
# Get summary of dataframe by year

# Data Dimensions, Column names, column types, preview of data
glimpse(df_year)
# Statistical summary of every column
summary(df_year)
# Print all column names
colnames(df_year)
```
The above dataset is grouped by year, with one row of observation per year from 1921-2020. The mean value of all the songs in a given year is used to group the various songs in each year. However, there seems to be an issue with the mode feature, which is 1 throughout the dataset. 


```{r}
# Get summary of dataframe by genre

# Data Dimensions, Column names, column types, preview of data
glimpse(df_genre)
# Statistical summary of every column
summary(df_genre)
# Print all column names
colnames(df_genre)
```
The above dataset is grouped by genre, with no time-series component. The mean value of all songs of a given genre is used to group the dataset.


```{r}
# Get summary of dataframe by artist

# Data Dimensions, Column names, column types, preview of data
glimpse(df_artist)
# Statistical summary of every column
summary(df_artist)
# Print all column names
colnames(df_artist)
```
The above dataset is grouped by artist, with no time-series component. The mean value of all songs of a given artist is used to group the dataset.
The "count" feature represents the number of songs published by that specific artist. 

```{r}
# Get summary of dataframe by genre and artist

# Data Dimensions, Column names, column types, preview of data
glimpse(df_genres2)
# Statistical summary of every column
summary(df_genres2)
# Print all column names
colnames(df_genres2)
```
The above dataset is grouped by artist, with no time-series component. The mean value of all songs of a given artist is used to group the dataset. For a given artist group, all relevant genres of the artists' published songs are stored as a list in the genres column. The "count" feature represents the number of songs published by that specific artist. 


## **Data Analysis** 

#### Look at distribution of data in the main dataset
```{r}
# Histogram of all numeric features
df %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

```{r}
# Density plot of all numeric features
df %>%
  keep(is.numeric) %>%                     
  gather() %>%                             
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +   
    geom_density() 
```
<br />

##### Insights: 
- The distribution of the duration of songs seems to stay largely within a certain range; the long right tail could be due to other audio products (e.g. audiobooks, podcasts, long instrumental songs)
- As expected, the distribution of explicitness and mode is bimodal because its values are binary, with songs being  categorized as 0 or 1 for each of these values.
- There are significant differences in the distribution of features in the given songs; for instance, songs tend to score low on instrumentalness and speechiness due to the high density near the lower side of the scale. In contrast, the songs seem to score along an almost uniform distribution for energy and valence.  
- The distribution of observations per year is also not uniform (with fewer observations in earlier years and in 2020), which confirms our initial description of this data frame as an unbalanced panel dataset. \n


#### Look at distribution of our key variable of interest, popularity
```{r}
popular_df <- data.frame(x = df$popularity)
Log_pol <- popular_df %>%
  ggplot(aes(x=x, fill = '#AA66FF'))+
  labs(title = "Density plot of Popularity", x = "Popularity", y = "Density")+
  geom_histogram(aes(y=..density..), color = 'black', fill='#AA66FF')+
  geom_density(aes(y=..density..),color = 'black',fill = 'grey',  alpha = 0.5,  kernel='gaussian')+
  geom_vline(aes(xintercept = mean(x)),color = 'red', linetype = 'dashed')+
  geom_vline(aes(xintercept = median(x)),color = 'blue', linetype = 'dashed')+
  geom_vline(aes(xintercept = quantile(x, probs = 0.25)),color = 'black')+
  geom_vline(aes(xintercept = quantile(x, probs = 0.75)),color = 'black')+
  theme_minimal()

Log_pol

```
<br />

Looking at the distribution for popularity, we have the blue dashed line representing the median and the red dashed line representing the mean. The two black lines represent the 0.25 and 0.75 quantiles, from left to right. The popularity density seems to be a bimodal distribution with a large number of songs with 0 popularity. This confirms our observations that a large quantity of older songs score low (or 0) in popularity. Otherwise, the distribution for popularity seems to have a large amount of kurtosis; this might be caused by the tendency for more recent songs (observations) to score higher in popularity, and also because Spotify is a fairly new platform (launched in 2008) that only gained a significant number of users in the past decade. \n


#### Look at overlapping distribution of key features in a 0 - 1 continuous scale
```{r}
correlated_density <- ggplot(df) +
    geom_density(aes(energy, fill ="energy", alpha = 0.1)) + 
    geom_density(aes(valence, fill ="valence", alpha = 0.1)) + 
    geom_density(aes(danceability, fill ="danceability", alpha = 0.1)) +
   geom_density(aes(acousticness, fill ="acousticness", alpha = 0.1)) + 
  geom_density(aes(liveness, fill ="liveness", alpha = 0.1)) + 
    scale_x_continuous(name = "Energy, Valence, Danceability, Acousticness, and Liveness") +
    scale_y_continuous(name = "Density") +
    ggtitle("Density plot of Energy, Valence, Danceability, Acousticness, and Liveness") +
    theme_bw() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          text = element_text(size = 12)) +
    theme(legend.title=element_blank()) +
    scale_fill_brewer(palette="Accent")

correlated_density

```
##### Insights: 
- This density plot overlaying the main features for the given songs enables us to see any potential relationships in the underlying distributions of these features.
- Acousticness has a bimodal distribution, with a large frequency of songs categorized at both ends of the spectrum.
- Liveness has a large frequency towards the lower end of the spectrum, but has a significant skew and long right tail.
- Danceability has a somewhat normal distribution but with large kurtosis.
- Valence and Energy have somewhat uniform distributions, with songs categorized evenly thoughout the spectrum. \n

#### Look at distribution of Songs by Key
```{r}
# Create key_val column to map key number to actual key
df$key <- as.character(df$key)
df$key_val <- revalue(df$key, c("0" = "C", "1" = "C♯,D♭", "2" = "D", "3" = "D♯,E♭", "4" = "E", "5" =  "F", "6" = "F♯,G♭","7" = "G","8" = "G♯,A♭","9" = "A","10" = "A♯,B♭","11" = "B"))
head(df$key_val)
```

```{r}
# Plot distribution of keys
song_keys <- df %>%
    group_by(key_val) %>%
    dplyr::summarise(n_key = n()) %>%
    arrange(desc(n_key))

song_keys$key <- factor(song_keys$key_val, levels = song_keys$key_val[order(song_keys$n_key)]) # in order to visualise the keys in descending order

ggplot(song_keys, aes(x = reorder(key,-n_key), y = n_key, fill = reorder(key,-n_key))) +
    geom_bar(stat = "identity") +
    labs(title = "Distribution by Key of Spotify Songs", x = "Keys", y = "Count of Key Frequency of Songs") +
    geom_text(aes(label=n_key), position = position_stack(vjust = 0.8)) +
    theme_bw() +
    theme(plot.title = element_text(size=15,face = "bold"), axis.title = element_text(size=12)) +
    theme(legend.position="none")
```
The above chart illustrates the fact that songs are composed using they keys C,G, and D most frequently. 


```{r}
# Plot Top Distribution of Keys for Top 10% of most popular songs
song_keys <- df %>%
    filter(quantile(popularity, 0.9)<popularity)%>% # Get Top Quantile of Songs
    group_by(key_val)%>%
    dplyr::summarise(n_key = n(), mean = mean(popularity)) %>%
    arrange(desc(n_key))
song_keys$key <- factor(song_keys$key_val, levels = song_keys$key_val[order(song_keys$n_key)]) # in order to visualise the keys in descending order

ggplot(song_keys, aes(x = reorder(key,-n_key), y = n_key, fill = reorder(key,-n_key))) +
    geom_bar(stat = "identity") +
    labs(title = "Distribution of the Keys of Top 10% most Popular Songs", x = "Keys", y = "Count of Key Frequency of Songs",subtitle="Top number represents frequency, bottom number represents mean popularity for that key") +
    geom_text(aes(label=n_key), position = position_stack(vjust = 0.8)) +
    geom_text(aes(label=round(mean)), position = position_stack(vjust = 0.2)) +
    theme_bw() +
    theme(plot.title = element_text(size=15,face = "bold"), axis.title = element_text(size=12)) +
    theme(plot.subtitle=element_text(size=10, hjust=0.5, face="italic", color="black"))+
    theme(legend.position="none")

```
This chart indicates that for the top 10% of the most popular songs, the most frequent key used is actually C sharp, C, and G. 



#### Look at feature changes over time
```{r, fig.width=10,fig.height=50}
# Group df by year, take the mean value of each feature
start_year = min(df$year)
end_year = max(df$year)
trend_chart <- function(arg){
trend_change <- df  %>% group_by(year) %>% summarize_at(vars(all_of(arg)), funs(Average = mean))
chart<- ggplot(data = trend_change, aes(x = as.Date(year), y = Average)) + 
     geom_line(color = "#00AFBB", size = 1)+ labs(x="Year")+ 
    scale_x_date(date_labels = "%Y") + scale_y_continuous(name=paste("",arg,sep=""))
return(chart)
}

# Plot trend over each year for each feature
trend_chart_track_popularity<-trend_chart("popularity")
trend_chart_acousticness <- trend_chart("acousticness")
trend_chart_danceability<-trend_chart("danceability")
trend_chart_energy<-trend_chart("energy")
trend_chart_loudness<-trend_chart("loudness")
trend_chart_valence <-trend_chart("valence")
trend_chart_tempo <-trend_chart("tempo")
trend_chart_duration_ms<-trend_chart("duration_ms")
trend_chart_speechiness<-trend_chart("speechiness")
trend_chart_liveness <-trend_chart("liveness")
trend_chart_instrumentalness <-trend_chart("instrumentalness")
trend_chart_explicit <- trend_chart("explicit")

plot_grid(trend_chart_track_popularity, trend_chart_danceability, trend_chart_energy, trend_chart_loudness, trend_chart_duration_ms, trend_chart_speechiness,trend_chart_acousticness,trend_chart_valence,trend_chart_tempo,trend_chart_liveness,trend_chart_instrumentalness,trend_chart_explicit, ncol = 1, label_size = 1)
```
##### Insights: 
- Average popularity of songs increases over time. This is expected as newer released songs are more likely to get more plays on Spotify. This may also highlight the trends and popularity towards pop music.
- Danceability, Loudness, Explicitness, Energy, and Tempo on average have trended up over time, highlighting that more recent songs score higher on these metrics.
- In contrast, Instrumentalness, Liveness, and Acousticness have clearly trended down over time, highlighting that more recent songs score lower on these metrics.
- There is no clear trend in the rest of the remaining features. \n



#### Look at Correlation of Features
```{r}
# Extract all numeric values from df
numerics = df[,!(colnames(df)  %in% c("id","artists","name","release_date", "key_val"))]
numerics$key = as.integer(numerics$key)
tidyverse_static_correlations <- numerics %>% correlate() 
print.data.frame(tidyverse_static_correlations)

```

```{r}
# Correlation Plot
mtCor <- cor(numerics)
corrplot(mtCor, method = "ellipse", type = "upper", tl.srt = 45)
```
It can be seen that energy is highly correlated with loudness, and that year is highly correlated with popularity. On the other hand, acousticness is highly inversely correlated with energy, year, popularity, and loudness.  

```{r}
# Network plot
# To interpret this plot, variables that are more highly correlated appear closer together and are joined by stronger paths. Paths are colored by their
# sign (positive = blue, red = negative).
net_plot <- tidyverse_static_correlations %>%
    network_plot(colours = c("indianred2", "black", "skyblue1"),repel=TRUE, legend = TRUE) +
    labs(
        title = "Network Plot of Correlations of Numeric Features of Spotify Songs"
        ) +
    expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +
    theme_tq() +
    theme(legend.position = "right")
net_plot

```
From this network plot, it can be seen that popularity, year, acousticness, loudness and energy are more strongly correlated as they are grouped together. There seems to also be a positive correlation between speechiness and explicitness. \n


#### Build a K-means Clustering model to further analyze the features

We use a K-Means Clustering model as an unsupervised learning technique to determine
how many natural "categories" of songs there are in the data. This is a dimensionality reduction technique used to group songs together.

We use the Elbow method to determine the optimal number of clusters, and we minimize the Within-cluster Sum-of-Squares.

```{r}
# Need to use all numeric values of relevant features and scale inputs 
cluster.input <- numerics
cols <- colnames(cluster.input)
cluster.input.scaled <- scale(cluster.input[,cols])
head(cluster.input.scaled)

```
Train clusters with various k-values (2-5)
```{r}
# kmeans with different k values
k2 <- kmeans(cluster.input.scaled, centers = 2, nstart = 25)
k3 <- kmeans(cluster.input.scaled, centers = 3, nstart = 25)
k4 <- kmeans(cluster.input.scaled, centers = 4, nstart = 25)
k5 <- kmeans(cluster.input.scaled, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point",  data = cluster.input.scaled) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = cluster.input.scaled) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = cluster.input.scaled) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = cluster.input.scaled) + ggtitle("k = 5")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```


```{r}
# Visualize how WSS-metric is reduced as we increase the number of clusters
set.seed(100)
fviz_nbclust(cluster.input[1:1000,], kmeans, method = "wss")
```

Generate a table of WSS errors by number of clusters
```{r}
n_clust<-fviz_nbclust(cluster.input[1:1000,], kmeans, method = "wss")
n_clust<-n_clust$data
n_clust %>% rename(replace = c("y" = "Within_sum_of_squared_error", "clusters" = "Number_of_clusters")) %>% 
  mutate(Within_sum_of_squared_error = color_tile("white", "red")(Within_sum_of_squared_error)) %>% 
  kable("html", escape = F) %>% 
  kable_styling("hover", full_width = F) %>% 
  column_spec(2, width = "5cm") %>%
  row_spec(3:3, bold = T, color = "white", background = "grey")
```

##### Insights: 
From this K-means cluster analysis, we can see that the within-cluster sum-of-squares error stabilizes when k becomes 3. 
Thus, this means that the features in the dataset can categorize the songs in 3 main categories; there is therefore a lot of commonalities from the features of the observed songs. 


#### Time-series Visualization of all features
##### Plotting annual median values for each feature overall. Standardized such that each feature is on the same scale.
```{r}

q1s = data.frame(matrix(ncol=3, nrow=0))
medians = data.frame(matrix(ncol=3, nrow=0))
q3s = data.frame(matrix(ncol=3, nrow=0))
feature_names = sort(c("acousticness", "danceability", "instrumentalness", "energy",
                  "duration_ms", "valence", "tempo", "liveness", "loudness", "speechiness"))
for(i in 1921:2020){
  annual_val = subset(df, year==i)

  for(name in feature_names){
    vals = quantile(annual_val[,name])[2:4]
    q1s <- rbind(q1s, c(i, name, vals[1]))
    medians <- rbind(medians, c(i, name, vals[2]))
    q3s <- rbind(q3s, c(i, name, vals[3]))
  }
}

colnames(q1s) <-c("Year", "Feature", "Value")
colnames(medians) <-c("Year", "Feature", "Value")
colnames(q3s) <-c("Year", "Feature", "Value")

split_q1s <- split(q1s, q1s$Feature)
split_medians <- split(medians, medians$Feature)
split_q3s <- split(q3s, q3s$Feature)

feature_quantiles = array(0, c(100,3,length(feature_names)))

for(i in 1:length(feature_names)){
  for(j in 1:100){
    feature_quantiles[j,1,i] = split_q1s[[i]][[3]][j]
    feature_quantiles[j,2,i] = split_medians[[i]][[3]][j]
    feature_quantiles[j,3,i] = split_q3s[[i]][[3]][j]
  }
}

dimnames(feature_quantiles) <- list(1921:2020,
                                    c("Q1", "Median", "Q3"),
                                    feature_names)



plot(1930,1, xlim = c(1921,2020), ylim = range(0,15), xlab="Year", ylab="Range")
for(i in 1:length(feature_names)){
  name = feature_names[i]
  medians = as.numeric(feature_quantiles[,,name][,"Median"])

  avg = mean(medians)
  medians = medians / avg
  
  lines(x=1921:2020, y=medians, col=i)
}
legend("topright", lty=1, col=c(1,2,3,4,5,6,7,8,9,10), legend=feature_names)
```


##### Plotting annual median values for the feature for the *top 10% of songs* by popularity. Standardized such that each feature is on the same scale.
```{r}
q1s = data.frame(matrix(ncol=3, nrow=0))
medians = data.frame(matrix(ncol=3, nrow=0))
q3s = data.frame(matrix(ncol=3, nrow=0))
feature_names = sort(c("acousticness", "danceability", "instrumentalness", "energy",
                  "duration_ms", "valence", "tempo", "liveness", "loudness", "speechiness"))
for(i in 1921:2020){
  annual_val = subset(df, year==i)
  
  
  top_10 = floor(nrow(annual_val)*0.1)
  annual_val = annual_val[order(annual_val$popularity, decreasing=TRUE),]
  annual_val = annual_val[1:top_10,]

  for(name in feature_names){
    vals = quantile(annual_val[,name])[2:4]
    q1s <- rbind(q1s, c(i, name, vals[1]))
    medians <- rbind(medians, c(i, name, vals[2]))
    q3s <- rbind(q3s, c(i, name, vals[3]))
  }
}

colnames(q1s) <-c("Year", "Feature", "Value")
colnames(medians) <-c("Year", "Feature", "Value")
colnames(q3s) <-c("Year", "Feature", "Value")

split_q1s <- split(q1s, q1s$Feature)
split_medians <- split(medians, medians$Feature)
split_q3s <- split(q3s, q3s$Feature)

feature_quantiles = array(0, c(100,3,length(feature_names)))

for(i in 1:length(feature_names)){
  for(j in 1:100){
    feature_quantiles[j,1,i] = split_q1s[[i]][[3]][j]
    feature_quantiles[j,2,i] = split_medians[[i]][[3]][j]
    feature_quantiles[j,3,i] = split_q3s[[i]][[3]][j]
  }
}

dimnames(feature_quantiles) <- list(1921:2020,
                                    c("Q1", "Median", "Q3"),
                                    feature_names)


plot(1930,1, xlim = c(1921,2020), ylim = range(0,17), xlab="Year", ylab="Range")
for(i in 1:length(feature_names)){
  name = feature_names[i]
  medians = as.numeric(feature_quantiles[,,name][,"Median"])
  avg = mean(medians)
  medians = medians / avg
  
  lines(x=1921:2020, y=medians, col=i)
}
legend("topright", lty=1, col=c(1,2,3,4,5,6,7,8,9,10), legend=feature_names)
```
#### Insights:  
Both these graphs appear to have similar shapes, with instrumentalness being the most widely varying, and the rest varying marginally. However, for the speechiness variable, for the overall set of songs its median value varies much less than the median value for the most popular songs.



##### Now plotting absolute values for each feature as the median values for overall, as well as just the most popular songs.
- MAPE is the difference between the most popular median and the overall median; R is the correlation between overall median and most popular median. \n
- These values will determine whether or not the overall median is a useful predictor of what the median for popular songs will be. This is important because it will be easier to forecast overall values into the future, given they are much smoother, and these forecast overall values can be used to predict the most popular feature value. \n
```{r}
# Get the list of features I need to work with

q1s = data.frame(matrix(ncol=3, nrow=0))
medians = data.frame(matrix(ncol=3, nrow=0))
best_medians = data.frame(matrix(ncol=3, nrow=0))
q3s = data.frame(matrix(ncol=3, nrow=0))
feature_names = sort(c("acousticness", "danceability", "instrumentalness", "energy",
                  "duration_ms", "valence", "tempo", "liveness", "loudness", "speechiness"))
for(i in 1921:2020){
  annual_val = subset(df, year==i)
  top_10 = floor(nrow(annual_val)*0.1)
  annual_val = annual_val[order(annual_val$popularity, decreasing=TRUE),]
  best_ones = annual_val[1:top_10,]

  for(name in feature_names){
    vals = quantile(annual_val[,name])[2:4]
    best_vals = quantile(best_ones[,name])[2:4]
    best_medians <- rbind(best_medians, c(i, name, best_vals[2]))
    medians <- rbind(medians, c(i, name, vals[2]))
  }
}

colnames(medians) <-c("Year", "Feature", "Value")
colnames(best_medians) <- c("Year", "Feature", "Value")

split_medians <- split(medians, medians$Feature)
split_best_medians <- split(best_medians, best_medians$Feature)

feature_quantiles = matrix(0, ncol=length(feature_names), nrow=100)
best_feature_quantiles = matrix(0, ncol=length(feature_names), nrow=100)

for(i in 1:length(feature_names)){
  for(j in 1:100){
    feature_quantiles[j,i] <- split_medians[[i]][[3]][j]
    best_feature_quantiles[j,i] = split_best_medians[[i]][[3]][j]
  }
}

dimnames(feature_quantiles) <- list(1921:2020,
                                    feature_names)
dimnames(best_feature_quantiles) <- list(1921:2020,
                                    feature_names)




for(i in 1:length(feature_names)){
  name = feature_names[i]
  scores = as.numeric(feature_quantiles[,i])
  best_scores = as.numeric(best_feature_quantiles[,i])
  mape = mape(best_scores, scores)
  mape = round(mape * 100,3)
  r  = cor(scores, best_scores)
  pval = t.test(scores, )
  # Normalize it????? / make it a function of its own mean
  
  smallest = min(min(scores), min(best_scores))
  biggest = max(max(scores), max(best_scores))
  
  plot(x=1921:2020, y=scores, col=1, type="l", ylim=c(smallest, biggest), main=name, sub=paste("MAPE: ", mape, "%;", "R:", r), xlab="Year", ylab="Median Value")
  lines(x=1921:2020, y=best_scores, col=2)
  legend("topright", lty=1, col=c(1,2), legend=c("Overall", "Most Popular"))
  title()
}
```
##### Insights: 

**Acousticness**: A high MAPE when comparing the most popular songs with the overall trends indicates that there is a large difference between the two (likely caused in the 1930s-1950s). The R value is high which means there is a high correlation between the most popular songs and the overall songs. Because acousticness seems to impact popular songs and there is a large change over time, we will move forward with predicting the acousticness of songs in the future. \n

**Danceability**: A low MAPE indicates not a lot of difference between popular songs and overall songs danceability. However, a high R value means there is a high correlation between the most popular songs and the overall songs. Because danceability seems to impact popular songs (due to the high correlation) and there is a some change over time, we will move forward with predicting the danceability of songs in the future. \n

**Duration (ms)**: There is a low MAPE which means that popular songs have about the same length as all other songs. The R value is relatively high which means there is a high correlation between the most popular songs and the overall songs. Because duration seems to impact popular songs (due to the high correlation) and there is a some change over time, we will move forward with predicting the duration of songs in the future. \n

**Energy**: A high MAPE when comparing the most popular songs with the overall trends indicates that there is a large difference between the two. The R value is high which means there is a high correlation between the most popular songs and the overall songs. Because energy seems to impact popular songs (due to the high MAPE and correlation) and there are large changes over time, we will move forward with predicting the energy of songs in the future. \n

**Instrumentalness**: The popular songs and overall music trends are pretty much the exact same after the 1950s, indicating that instrumentalness does not affect popularity. Because instrumentalness does not impact popular songs and there is a flatline after the 1950s (indicating no change over time), we will not move forward with predicting the instrumentalness of songs in the future. \n

**Liveness**: A high MAPE when comparing the most popular songs with the overall trends indicates that there is a large difference between the two. The R value is high which means there is a high correlation between the most popular songs and the overall songs. Because liveness seems to impact popular songs (due to the high MAPE and correlation) and there is a some change over time, we will move forward with predicting the liveness of songs in the future. \n

**Loudness**: A high MAPE when comparing the most popular songs with the overall trends indicates that there is a large difference between the two. The R value is high which means there is a high correlation between the most popular songs and the overall songs. Because loudness seems to impact popular songs (due to the high MAPE and correlation) and there is a significant change over time, we will move forward with predicting the loudness of songs in the future. \n

**Speechiness**: A high MAPE when comparing the most popular songs with the overall trends indicates that there is a large difference between the two. It's important to note that the driver of the high MAPE is due to the two large spikes between 1930 and 1950, while speechiness levels between popular songs and the overall industry in the last 70 years have generally been the same. The R value is low which means there is a low correlation between the most popular songs and the overall songs. We will not move forward with predicting speechiness, due to the lack of change in speechiness over time and low R. \n

**Tempo**: The MAPE is low, inidcating that there isn't a significant difference in tempo between the popular songs and the overall music trends. The R is relatively high, so there is possibly a correlation between popular songs and the overall songs. Because tempo seems to impact popular songs (due to the high correlation) and there is a some change over time, we will move forward with predicting the tempo of songs in the future. \n

**Valence**: A high MAPE when comparing the most popular songs with the overall trends indicates that there is a large difference between the two in terms of valence. The R value is high which means there is a high correlation between the most popular songs and the overall songs. Because valence seems to impact popular songs (due to the high MAPE and high correlation) and there is a some change over time, we will move forward with predicting the valence of songs in the future. \n


## **Model Building** 

### Timeseries trend Predictions
#### Train time series models to forecast future feature trends
```{r}
# Set up the libraries and the training/testing amounts
training.percent = 0.95
nTrain = 100*training.percent
nTest = 100*(1-training.percent)
```
<br />

#### Acousticness Models

```{r}
acousticness = ts(as.numeric(feature_quantiles[,"acousticness"]) , start=1921)
acousticness.train = subset(acousticness, start=1, end=nTrain)
acousticness.test = subset(acousticness, start = (nTrain+1), end =(nTrain+nTest))

#ses
acousticness.train.ses <- ses(acousticness.train, h=nTest)
acousticness.ses.mape = mape(acousticness.train.ses$mean, acousticness.test)
plot(acousticness.train.ses, main=paste("Acousticness", "SES"), sub=paste("MAPE:", round(acousticness.ses.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(acousticness)

#SARIMA
acousticness.sarima.model <- auto.arima(acousticness.train)
acousticness.sarima <- forecast(acousticness.sarima.model, h=nTest)
acousticness.sarima.mape = mape(acousticness.sarima$mean, acousticness.test)
plot(acousticness.sarima, main=paste("Acousticness", "SARIMA"), sub=paste("MAPE:", round(acousticness.sarima.mape*100,3), "%"), xlab="Year", ylab="Median Value")
lines(acousticness)

#Neural Network
acousticness.train.nn <- elm(acousticness.train)
acousticness.nn.forecast <- forecast(acousticness.train.nn, h=nTest)
acousticness.nn.mape = mape(acousticness.nn.forecast$mean, acousticness.test)
plot(acousticness, main=paste("Acousticness", "NN"), sub=paste("MAPE:", round(acousticness.nn.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(acousticness.nn.forecast$mean, start=1921+nTrain), col=2)
```
Here we see that the best method to predict future acousticness median values for the overall data set is through SES. However, given that this best MAPE is roughly 37%, this feature cannot be reliably forecast into the future.
  


<br />

#### Energy models
```{r}
energy = ts(as.numeric(feature_quantiles[,"energy"]) , start=1921)
energy.train = subset(energy, start=1, end=nTrain)
energy.test = subset(energy, start = (nTrain+1), end =(nTrain+nTest))

#SES
energy.train.ses <- ses(energy.train, h=nTest)
energy.ses.mape = mape(energy.train.ses$mean, energy.test)
plot(energy.train.ses, main=paste("Energy", "SES"), sub=paste("MAPE:", round(energy.ses.mape* 100, 3), "%"), xlab="Year", ylab="Median Value")
lines(energy)

#SARIMA
energy.sarima.model <- auto.arima(energy.train)
energy.sarima <- forecast(energy.sarima.model, h=nTest)
energy.sarima.mape = mape(energy.sarima$mean, energy.test)
plot(energy.sarima, main=paste("Energy", "SARIMA"), sub=paste("MAPE:", round(energy.sarima.mape* 100, 3), "%"), xlab="Year", ylab="Median Value")
lines(energy)

#Neural Network
energy.train.nn <- elm(energy.train)
energy.nn.forecast <- forecast(energy.train.nn, h=nTest)
energy.nn.mape = mape(energy.nn.forecast$mean, energy.test)
plot(energy, main=paste("Energy", "NN"), sub=paste("MAPE:", round(energy.nn.mape* 100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(energy.nn.forecast$mean, start=1921+nTrain), col=2)
```
Here we see that the best method to predict future energy median values for the overall data set is through SES. Given that this best MAPE is roughly 8%, this feature can be fairly reliably forecast into the future.
  
<br />

#### Danceability models
```{r}
danceability = ts(as.numeric(feature_quantiles[,"danceability"]) , start=1921)
danceability.train = subset(danceability, start=1, end=nTrain)
danceability.test = subset(danceability, start = (nTrain+1), end =(nTrain+nTest))

#SES
danceability.train.ses <- ses(danceability.train, h=nTest)
danceability.ses.mape = mape(danceability.train.ses$mean, danceability.test)
plot(danceability.train.ses, main=paste("danceability", "SES"), sub=paste("MAPE:", round(danceability.ses.mape* 100, 3), "%"), xlab="Year", ylab="Median Value")
lines(danceability)

#SARIMA
danceability.sarima.model <- auto.arima(danceability.train)
danceability.sarima <- forecast(danceability.sarima.model, h=nTest)
danceability.sarima.mape = mape(danceability.sarima$mean, danceability.test)
plot(danceability.sarima, main=paste("danceability", "SARIMA"), sub=paste("MAPE:", round(danceability.sarima.mape* 100, 3), "%"), xlab="Year", ylab="Median Value")
lines(danceability)

#Neural Network
danceability.train.nn <- elm(danceability.train)
danceability.nn.forecast <- forecast(danceability.train.nn, h=nTest)
danceability.nn.mape = mape(danceability.nn.forecast$mean, danceability.test)
plot(danceability, main=paste("danceability", "NN"), sub=paste("MAPE:", round(danceability.nn.mape* 100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(danceability.nn.forecast$mean, start=1921+nTrain), col=2)
```
Here we see that the best method to predict future danceability median values for the overall data set is through NN. However, given that this best MAPE is roughly 10.4%, this feature cannot be reliably forecast into the future.
  
<br />

#### Duration models

```{r}
duration = ts(as.numeric(feature_quantiles[,"duration_ms"]) , start=1921)
duration.train = subset(duration, start=1, end=nTrain)
duration.test = subset(duration, start = (nTrain+1), end =(nTrain+nTest))

#SES
duration.train.ses <- ses(duration.train, h=nTest)
duration.ses.mape = mape(duration.train.ses$mean, duration.test)
plot(duration.train.ses, main=paste("Duration", "SES"), sub=paste("MAPE:",round(duration.ses.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(duration)

#SARIMA
duration.sarima.model <- auto.arima(duration.train)
duration.sarima <- forecast(duration.sarima.model, h=nTest)
duration.sarima.mape = mape(duration.sarima$mean, duration.test)
plot(duration.sarima, main=paste("Duration", "SARIMA"), sub=paste("MAPE:", round(duration.sarima.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(duration)


#Neural Network
duration.train.nn <- elm(duration.train)
duration.nn.forecast <- forecast(duration.train.nn, h=nTest)
duration.nn.mape = mape(duration.nn.forecast$mean, duration.test)
plot(duration, main=paste("Duration", "NN"), sub=paste("MAPE:", round(duration.nn.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(duration.nn.forecast$mean, start=1921+nTrain), col=2)
```
Here we see that the best method to predict future duration median values for the overall data set is through NN. Given that this best MAPE is roughly 8%, this feature can be fairly reliably forecast into the future.
  
<br />

#### Valence
```{r}
valence = ts(as.numeric(feature_quantiles[,"valence"]) , start=1921)
valence.train = subset(valence, start=1, end=nTrain)
valence.test = subset(valence, start = (nTrain+1), end =(nTrain+nTest))

#SES
valence.train.ses <- ses(valence.train, h=nTest)
valence.ses.mape = mape(valence.train.ses$mean, valence.test)
plot(valence.train.ses, main=paste("valence", "SES"), sub=paste("MAPE:",round(valence.ses.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(valence)

#SARIMA
valence.sarima.model <- auto.arima(valence.train)
valence.sarima <- forecast(valence.sarima.model, h=nTest)
valence.sarima.mape = mape(valence.sarima$mean, valence.test)
plot(valence.sarima, main=paste("valence", "SARIMA"), sub=paste("MAPE:", round(valence.sarima.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(valence)


#Neural Network
valence.train.nn <- elm(valence.train)
valence.nn.forecast <- forecast(valence.train.nn, h=nTest)
valence.nn.mape = mape(valence.nn.forecast$mean, valence.test)
plot(valence, main=paste("valence", "NN"), sub=paste("MAPE:", round(valence.nn.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(valence.nn.forecast$mean, start=1921+nTrain), col=2)
```
##### Insights:
Relatively lower MAPE indicates that we can predict valence over time. The best model to use is the SARIMA model because it has the lowest MAPE at 6.7%. 

<br />

#### Tempo model

```{r}
tempo = ts(as.numeric(feature_quantiles[,"tempo"]) , start=1921)
tempo.train = subset(tempo, start=1, end=nTrain)
tempo.test = subset(tempo, start = (nTrain+1), end =(nTrain+nTest))

#SES
tempo.train.ses <- ses(tempo.train, h=nTest)
tempo.ses.mape = mape(tempo.train.ses$mean, tempo.test)
plot(tempo.train.ses, main=paste("tempo", "SES"), sub=paste("MAPE:",round(tempo.ses.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(tempo)

#SARIMA
tempo.sarima.model <- auto.arima(tempo.train)
tempo.sarima <- forecast(tempo.sarima.model, h=nTest)
tempo.sarima.mape = mape(tempo.sarima$mean, tempo.test)
plot(tempo.sarima, main=paste("tempo", "SARIMA"), sub=paste("MAPE:", round(tempo.sarima.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(tempo)


#Neural Network
tempo.train.nn <- elm(tempo.train)
tempo.nn.forecast <- forecast(tempo.train.nn, h=nTest)
tempo.nn.mape = mape(tempo.nn.forecast$mean, tempo.test)
plot(tempo, main=paste("tempo", "NN"), sub=paste("MAPE:", round(tempo.nn.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(tempo.nn.forecast$mean, start=1921+nTrain), col=2)
```
##### Insights:
Here we see that the best method to predict future tempo median values for the overall data set is through SES. Given that this best MAPE is roughly 1%, this feature can be fairly reliably forecast into the future.
  
<br />

#### Liveness
```{r}
liveness = ts(as.numeric(feature_quantiles[,"liveness"]) , start=1921)
liveness.train = subset(liveness, start=1, end=nTrain)
liveness.test = subset(liveness, start = (nTrain+1), end =(nTrain+nTest))

#SES
liveness.train.ses <- ses(liveness.train, h=nTest)
liveness.ses.mape = mape(liveness.train.ses$mean, liveness.test)
plot(liveness.train.ses, main=paste("liveness", "SES"), sub=paste("MAPE:",round(liveness.ses.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(liveness)

#SARIMA
liveness.sarima.model <- auto.arima(liveness.train)
liveness.sarima <- forecast(liveness.sarima.model, h=nTest)
liveness.sarima.mape = mape(liveness.sarima$mean, liveness.test)
plot(liveness.sarima, main=paste("liveness", "SARIMA"), sub=paste("MAPE:", round(liveness.sarima.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(liveness)


#Neural Network
liveness.train.nn <- elm(liveness.train)
liveness.nn.forecast <- forecast(liveness.train.nn, h=nTest)
liveness.nn.mape = mape(liveness.nn.forecast$mean, liveness.test)
plot(liveness, main=paste("liveness", "NN"), sub=paste("MAPE:", round(liveness.nn.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(liveness.nn.forecast$mean, start=1921+nTrain), col=2)
```
##### Insights:
Here we see that the best method to predict future liveness median values for the overall data set is through SARIMA Given that this best MAPE is roughly 2.6%, this feature can be fairly reliably forecast into the future.

<br />

#### Loudness
```{r}
loudness = ts(as.numeric(feature_quantiles[,"loudness"]) , start=1921)
loudness.train = subset(loudness, start=1, end=nTrain)
loudness.test = subset(loudness, start = (nTrain+1), end =(nTrain+nTest))

#SES
loudness.train.ses <- ses(loudness.train, h=nTest)
loudness.ses.mape = mape(loudness.train.ses$mean, loudness.test)
plot(loudness.train.ses, main=paste("loudness", "SES"), sub=paste("MAPE:",round(loudness.ses.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(loudness)

#SARIMA
loudness.sarima.model <- auto.arima(loudness.train)
loudness.sarima <- forecast(loudness.sarima.model, h=nTest)
loudness.sarima.mape = mape(loudness.sarima$mean, loudness.test)
plot(loudness.sarima, main=paste("loudness", "SARIMA"), sub=paste("MAPE:", round(loudness.sarima.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(loudness)


#Neural Network
loudness.train.nn <- elm(loudness.train)
loudness.nn.forecast <- forecast(loudness.train.nn, h=nTest)
loudness.nn.mape = mape(loudness.nn.forecast$mean, loudness.test)
plot(loudness, main=paste("loudness", "NN"), sub=paste("MAPE:", round(loudness.nn.mape*100, 3), "%"), xlab="Year", ylab="Median Value")
lines(ts(loudness.nn.forecast$mean, start=1921+nTrain), col=2)
```
##### Insights:
Here we see that the best method to predict future loudness median values for the overall data set is through SES Given that this best MAPE is roughly 5%, this feature can be fairly reliably forecast into the future.

<br />

#### Linear Model for Overall Feature Median--> Most Popular Songs' Feature Median

We will only consider models that had a low enough MAPEs (10% chosen as a threshold) in their best timeseries forecasting model. \n

Here, MAPE is the difference between the actual median for most popular songs and the predicted median.

```{r}
#Energy
best_energy = ts(as.numeric(best_feature_quantiles[,"energy"]), start=1921, end=2020) 
best_energy.train = subset(best_energy, start = 1, end = nTrain)
best_energy.test = subset(best_energy, start = nTrain+1, end = nTrain+nTest)
train_df = data.frame(best = best_energy.train, overall = energy.train)

best = best_energy.train
overall = energy.train

energy.lm <- lm(best ~ overall)
test_df <- data.frame(overall = energy.test)
best_energy.test.predict <- predict(energy.lm, newdata=test_df, interval="prediction")
best_energy_test_predict.mape <- mape(best_energy.test[1:length(best_energy.test)], best_energy.test.predict[,1])
plot(best_energy,xlab="Year", ylab="Energy", main=paste("Energy Popularity Prediction"), sub = paste("MAPE: ", round(best_energy_test_predict.mape*100, 3), "%"))
lines(ts(best_energy.test.predict[,1], start = 1921+nTrain+1, end = 2020), col = 2)

#Duration
best_duration = ts(as.numeric(best_feature_quantiles[,"duration_ms"]), start=1921, end=2020) 
best_duration.train = subset(best_duration, start = 1, end = nTrain)
best_duration.test = subset(best_duration, start = nTrain+1, end = nTrain+nTest)
train_df = data.frame(best = best_duration.train, overall = duration.train)

best = best_duration.train
overall = duration.train

duration.lm <- lm(best ~ overall)
test_df <- data.frame(overall = duration.test)
best_duration.test.predict <- predict(duration.lm, newdata=test_df, interval="prediction")
best_duration_test.predict.mape <- mape(best_duration.test[1:length(best_duration.test)], best_duration.test.predict[,1])
plot(best_duration,xlab="Year", ylab="duration", main=paste("duration Popularity Prediction"), sub = paste("MAPE: ", round(best_duration_test.predict.mape*100, 3), "%"))
lines(ts(best_duration.test.predict[,1], start = 1921+nTrain+1, end = 2020), col = 2)

# Acousticness
best_acousticness = ts(as.numeric(best_feature_quantiles[,"acousticness"]), start=1921, end=2020) 
best_acousticness.train = subset(best_acousticness, start = 1, end = nTrain)
best_acousticness.test = subset(best_acousticness, start = nTrain+1, end = nTrain+nTest)
train_df = data.frame(best = best_acousticness.train, overall = acousticness.train)

best = best_acousticness.train
overall = acousticness.train

acousticness.lm <- lm(best ~ overall)
test_df <- data.frame(overall = acousticness.test)
best_acousticness.test.predict <- predict(acousticness.lm, newdata=test_df, interval="prediction")
best_acousticness_test.predict.mape <- mape(best_acousticness.test[1:length(best_acousticness.test)], best_acousticness.test.predict[,1])
plot(best_acousticness,xlab="Year", ylab="acousticness", main=paste("acousticness Popularity Prediction"), sub = paste("MAPE: ", round(best_acousticness_test.predict.mape*100, 3), "%"))
lines(ts(best_acousticness.test.predict[,1], start = 1921+nTrain+1, end = 2020), col = 2)

#Valence
best_valence = ts(as.numeric(best_feature_quantiles[,"valence"]), start=1921, end=2020) 
best_valence.train = subset(best_valence, start = 1, end = nTrain)
best_valence.test = subset(best_valence, start = nTrain+1, end = nTrain+nTest)
train_df = data.frame(best = best_valence.train, overall = valence.train)

best = best_valence.train
overall = valence.train

valence.lm <- lm(best ~ overall)
test_df <- data.frame(overall = valence.test)
best_valence.test.predict <- predict(valence.lm, newdata=test_df, interval="prediction")
best_valence_test.predict.mape <- mape(best_valence.test[1:length(best_valence.test)], best_valence.test.predict[,1])
plot(best_valence,xlab="Year", ylab="Valence", main=paste("Valence Popularity Prediction"), sub = paste("MAPE: ", round(best_valence_test.predict.mape*100, 3), "%"))
lines(ts(best_valence.test.predict[,1], start = 1921+nTrain+1, end = 2020), col = 2)

#Tempo
best_tempo = ts(as.numeric(best_feature_quantiles[,"tempo"]), start=1921, end=2020) 
best_tempo.train = subset(best_tempo, start = 1, end = nTrain)
best_tempo.test = subset(best_tempo, start = nTrain+1, end = nTrain+nTest)
train_df = data.frame(best = best_tempo.train, overall = tempo.train)

best = best_tempo.train
overall = tempo.train

tempo.lm <- lm(best ~ overall)
test_df <- data.frame(overall = tempo.test)
best_tempo.test.predict <- predict(tempo.lm, newdata=test_df, interval="prediction")
best_tempo_test.predict.mape <- mape(best_tempo.test[1:length(best_tempo.test)], best_tempo.test.predict[,1])
plot(best_tempo,xlab="Year", ylab="tempo", main=paste("tempo Popularity Prediction"), sub = paste("MAPE: ", round(best_tempo_test.predict.mape*100, 3), "%"))
lines(ts(best_tempo.test.predict[,1], start = 1921+nTrain+1, end = 2020), col = 2)

#Liveness
best_liveness = ts(as.numeric(best_feature_quantiles[,"liveness"]), start=1921, end=2020) 
best_liveness.train = subset(best_liveness, start = 1, end = nTrain)
best_liveness.test = subset(best_liveness, start = nTrain+1, end = nTrain+nTest)
train_df = data.frame(best = best_liveness.train, overall = liveness.train)

best = best_liveness.train
overall = liveness.train

liveness.lm <- lm(best ~ overall)
test_df <- data.frame(overall = liveness.test)
best_liveness.test.predict <- predict(liveness.lm, newdata=test_df, interval="prediction")
best_liveness_test.predict.mape <- mape(best_liveness.test[1:length(best_liveness.test)], best_liveness.test.predict[,1])
plot(best_liveness,xlab="Year", ylab="liveness", main=paste("liveness Popularity Prediction"), sub = paste("MAPE: ", round(best_liveness_test.predict.mape*100, 3), "%"))
lines(ts(best_liveness.test.predict[,1], start = 1921+nTrain+1, end = 2020), col = 2)

#Loudness
best_loudness = ts(as.numeric(best_feature_quantiles[,"loudness"]), start=1921, end=2020) 
best_loudness.train = subset(best_loudness, start = 1, end = nTrain)
best_loudness.test = subset(best_loudness, start = nTrain+1, end = nTrain+nTest)
train_df = data.frame(best = best_loudness.train, overall = loudness.train)

best = best_loudness.train
overall = loudness.train

loudness.lm <- lm(best ~ overall)
test_df <- data.frame(overall = loudness.test)
best_loudness.test.predict <- predict(loudness.lm, newdata=test_df, interval="prediction")
best_loudness_test.predict.mape <- mape(best_loudness.test[1:length(best_loudness.test)], best_loudness.test.predict[,1])
plot(best_loudness,xlab="Year", ylab="loudness", main=paste("loudness Popularity Prediction"), sub = paste("MAPE: ", round(best_loudness_test.predict.mape*100, 3), "%"))
lines(ts(best_loudness.test.predict[,1], start = 1921+nTrain+1, end = 2020), col = 2)
```
##### Insights:
**Energy**: A low MAPE around 4% indicates that we can use the overall trend to identify the energy of popular songs \n

**Duration**: A low MAPE around 1% indicates that we can use the overall trend to identify the duration of popular songs \n

**Acousticness**: A high MAPE around 16% indicates that we can not use the overall trend to identify the acousticness of popular songs. We will not include predictions for acousticness of popular songs. \n

**Valence**: A low MAPE around 4.6% indicates that we can use the overall trend to identify the valence of popular songs \n

**Tempo**: A low MAPE around 2% indicates that we can use the overall trend to identify the tempo of popular songs \n

**Liveness**: A low MAPE around 4.3% indicates that we can use the overall trend to identify the liveness of popular songs \n

**Loudness**: A low MAPE around 5% indicates that we can use the overall trend to identify the loudness of popular songs \n

<br />

### Predict features for the overall market trends for the next five years.
Only including features whose trained linear models had a MAPE below 10%, as these are the features whose most popular medians will be most reliably predicted by their forecast overall values. \n
Using the best timeseries method seen for each feature.

<br />

#### Energy: SES Model

```{r}
energy.ses <- ses(energy, h=nTest)
plot(energy.ses, main=paste("Energy", "SES"), xlab="Year", ylab="Median Value")
lines(energy)
```
<br />

#### Duration: NN Model

```{r}
duration.nn <- elm(duration)
duration.nn.predict <- forecast(duration.nn, h=nTest)
plot(duration.nn.predict, main=paste("Duration", "NN"), xlab="Year", ylab="Median Value")
```
<br />

#### Tempo: SES Model 

```{r}
tempo.ses <- ses(tempo, h=nTest)
plot(tempo.ses, main=paste("Tempo", "SES"), xlab="Year", ylab="Median Value")
lines(tempo)
```
<br />

#### Valence: SARIMA Model

```{r}
valence.sarima <- auto.arima(valence)
valence.sarima.predict <- forecast(valence.sarima, h=nTest)
plot(valence.sarima.predict, main=paste("Valence", "SARIMA"), xlab="Year", ylab="Median Value")
lines(valence)
```
<br />

#### Liveness: SARIMA Model

```{r}
liveness.sarima <- auto.arima(liveness)
liveness.sarima.predict <- forecast(liveness.sarima, h=nTest)
plot(liveness.sarima.predict, main=paste("liveness", "SARIMA"), xlab="Year", ylab="Median Value")
lines(liveness)
```
<br />

#### Loudness: SES Model 

```{r}
loudness.ses <- ses(loudness, h=nTest)
plot(loudness.ses, main=paste("loudness", "SES"), xlab="Year", ylab="Median Value")
lines(loudness)
```





### Now predicting the median values for the *most popular songs* for this set of features.
The forecast overall median values and linear model predicting the most popular median values for each feature given overall median value will be used to predict the future median values for popular songs for each feature.

<br />

#### Energy

```{r}
best = best_energy
overall = energy


energy.lm <- lm(best ~ overall)
predict_df <- data.frame(overall = energy.ses$mean)
energy.predict <- predict(energy.lm, newdata=predict_df, interval="prediction")

smallest = min(min(best), min(overall))
biggest = max(max(best), max(overall))
plot(energy, xlab="Year", ylab="Median Value", ylim=c(smallest, biggest), main="Most Popular Song's Energy Feature Prediction", sub=paste("Projected median feature value for most popular songs:", paste(round(energy.ses$mean, 3), collapse=",") ))
lines(best_energy, col = 2)
lines(ts(energy.predict[,1], start=2020+1, end=2020+nTest), col = 3)
legend("bottomright", lty=1, col=c(1,2,3), legend=c("Overall", "Most Popular", "Most Popular Projection"))
```

<br />

#### Duration

```{r}
best = best_duration
overall = duration

duration.lm <- lm(best ~ overall)
predict_df <- data.frame(overall = duration.nn.predict$mean)
duration.predict <- predict(duration.lm, newdata=predict_df, interval="prediction")

smallest = min(min(best), min(overall))
biggest = max(max(best), max(overall))
plot(duration, xlab="Year", ylab="Median Value", ylim=c(smallest, biggest), main="Most Popular Song's Duration Feature Prediction", sub=paste("Projected median feature value for most popular songs:", paste(round(duration.nn.predict$mean), collapse=",") ))
lines(best_duration, col = 2)
lines(ts(duration.predict[,1], start=2020+1, end=2020+nTest), col = 3)
legend("topright", lty=1, col=c(1,2,3), legend=c("Overall", "Most Popular", "Most Popular Projection"))
```

<br />

#### Tempo

```{r}
best = best_tempo
overall = tempo

tempo.lm <- lm(best ~ overall)
predict_df <- data.frame(overall = tempo.ses$mean)
tempo.predict <- predict(tempo.lm, newdata=predict_df, interval="prediction")


smallest = min(min(best), min(overall))
biggest = max(max(best), max(overall))
plot(tempo, xlab="Year", ylab="Median Value", ylim=c(smallest, biggest), main="Most Popular Song's Tempo Feature Prediction", sub=paste("Projected median feature value for most popular songs:", paste(round(tempo.ses$mean), collapse=",") ))
lines(best_tempo, col = 2)
lines(ts(tempo.predict[,1], start=2020+1, end=2020+nTest), col = 3)
legend("bottomright", lty=1, col=c(1,2,3), legend=c("Overall", "Most Popular", "Most Popular Projection"))
```

<br />

#### Valence

```{r}
best = best_valence
overall = valence

valence.lm <- lm(best ~ overall)
predict_df <- data.frame(overall = valence.sarima.predict$mean)
valence.predict <- predict(valence.lm, newdata=predict_df, interval="prediction")


smallest = min(min(best), min(overall))
biggest = max(max(best), max(overall))
plot(valence, xlab="Year", ylab="Median Value", ylim=c(smallest, biggest), main="Most Popular Song's valence Feature Prediction", sub=paste("Projected median feature value for most popular songs:", paste(round(valence.sarima.predict$mean,3), collapse=",") ))
lines(best_valence, col = 2)
lines(ts(valence.predict[,1], start=2020+1, end=2020+nTest), col = 3)
legend("bottomright", lty=1, col=c(1,2,3), legend=c("Overall", "Most Popular", "Most Popular Projection"))
```

<br />

#### Liveness

```{r}
best = best_liveness
overall = liveness

liveness.lm <- lm(best ~ overall)
predict_df <- data.frame(overall = liveness.sarima.predict$mean)
liveness.predict <- predict(liveness.lm, newdata=predict_df, interval="prediction")

smallest = min(min(best), min(overall))
biggest = max(max(best), max(overall))
plot(liveness, xlab="Year", ylab="Median Value", ylim=c(smallest, biggest), main="Most Popular Song's Liveness Feature Prediction", sub=paste("Projected median feature value for most popular songs:", paste(round(liveness.sarima.predict$mean,3), collapse=",") ))
lines(best_liveness, col = 2)
lines(ts(liveness.predict[,1], start=2020+1, end=2020+nTest), col = 3)
legend("topright", lty=1, col=c(1,2,3), legend=c("Overall", "Most Popular", "Most Popular Projection"))
```


<br />

#### Loudness

```{r}
best = best_loudness
overall = loudness

loudness.lm <- lm(best ~ overall)
predict_df <- data.frame(overall = loudness.ses$mean)
loudness.predict <- predict(loudness.lm, newdata=predict_df, interval="prediction")

smallest = min(min(best), min(overall))
biggest = max(max(best), max(overall))
plot(loudness, xlab="Year", ylab="Median Value", ylim=c(smallest, biggest), main="Most Popular Song's Loudness Feature Prediction", sub=paste("Projected median feature value for most popular songs:", paste(round(loudness.ses$mean,1), collapse=",") ))
lines(best_loudness, col = 2)
lines(ts(loudness.predict[,1], start=2020+1, end=2020+nTest), col = 3)
legend("bottomright", lty=1, col=c(1,2,3), legend=c("Overall", "Most Popular", "Most Popular Projection"))
```

<br />

### Popularity Predictions

#### Transformations to create dataset for predictions

Look at distribution of main dataset by year
```{r}
histogram(df$year)
```
The dataset has an even quantity of data for each year since 1960 (except for 2020). 
As a result, in order to reduce the burden of the dataset for predictions, we truncate the dataset to rows on and after the year 1990. 

```{r}
# Apply transformations to create a prediction dataset

# Truncate to years on or after 1990
predict_set = df[df$year >= 1990,]

# Remove uneeded columns
predict_set = predict_set[,!(colnames(predict_set)  %in% c("id","release_date","name", "artists"))]
glimpse(predict_set)
```
Key is categorical - need to turn the feature into multiple columns of dummy variables 
```{r}
# Look at number of unique Keys
unique(predict_set$key)
```

```{r}
# Create dummy variables for each unique key value.
# remove_first_dummy set to true to remove first dummy variable created - done to avoid multicollinearity in a multiple regression model. 
predict_set <- fastDummies::dummy_cols(predict_set,select_columns = "key", remove_first_dummy = TRUE)
predict_set = select(predict_set, -c(key,key_val))
head(predict_set)
```

#### Split into training and prediction set (80/20 split)

```{r}
# 80% as train_set
train_size <- floor(0.80 * nrow(predict_set))

set.seed(123)
train_ind <- sample(seq_len(nrow(predict_set)), size = train_size)

predict_set.train <- predict_set[train_ind, ]
predict_set.test <- predict_set[-train_ind, ]

# Look at number of prediction/training set rows
nrow(predict_set.train)
nrow(predict_set.test)
```

```{r}
# Snapshot of training set
glimpse(predict_set.train)
summary(predict_set.train)
```

#### Train a Multiple Linear Regression Model to predict popularity
```{r}
predict_set.lm_train <- glm(popularity~., data=predict_set.train)
# Get training results
summary(predict_set.lm_train)
```
Test model on predictions and get training errors
```{r}
predictions.lm <- predict(predict_set.lm_train,predict_set.test)
paste("RMSE:",Metrics::rmse(predict_set.test$popularity, predictions.lm))
paste("MAE:",Metrics::mae(predict_set.test$popularity, predictions.lm))
paste("MASE:",Metrics::mase(predict_set.test$popularity, predictions.lm))
```

##### Error Metrics:
RMSE: Root-mean squared error is used as a main error metric to minimize. This error metric provides a measure of accuracy of the model that is dependent on the scale of the dependent variable that we seek to predict (i.e. popularity, which ranges from 0 - 100).\n

MAE: Mean absolute error is another error metric used, as a measure of errors between paired observations and the (average) accuracy of the model. \n

MASE: Note that Mean Absolute Scaled Error (MASE) is used instead of MAPE since MASE is scale invariant (mean absolute scaled error is independent of the scale of the data, so it can be used to compare forecasts across datasets with different scales). Also, MASE does not cause issues when y is close to 0 (or y is 0), which occurs in the test dataset (since some songs have popularity equal to 0), and which can cause MAPE to go to infinity.\n


##### Insights:
- From this regression, it seems like year, speechiness, explicitness, danceability, valence, liveness, and instrumentalness are significant predictors of popularity, judging by their t-stats and p-values. 
- However, there remains a lot of regressors that are insignificant predictors of popularity (e.g. key_3, key_10, key_7, tempo).
- Furthermore, the RMSE suggests that the model is capable or predicting popularity within +/- 9 points of error (remember that popularity is contained within a scale of 0-100. MAE suggests that this error is ~ +/- 7 points. This is still a significant deviation from the correct popularity value. \n


#### Train a Random Forests Model to predict popularity
In order to capture the non-linear relationships in the dataset, we train a random-forests model.
This model can capture non-linear relationships by utilizing various independent decision trees to arrive at a prediction.

We speed up this process, we train this random-forest with 500 iterations in parallel, using 8-cores. 

```{r}
# Change the below parameter to change the number of cores to parallelize on. 
cl<-makePSOCKcluster(8) 
  
registerDoParallel(cl) 
  
start.time<-proc.time() 
  
 
set.seed(123)
predict_set.rf_train = randomForest(predict_set.train[, !names(predict_set.train) %in% c("popularity")],predict_set.train$popularity,xtest=predict_set.test[, !names(predict_set.train) %in% c("popularity")],ytest=predict_set.test$popularity,do.trace=TRUE, keep.forest=TRUE)

predict_set.rf_train = randomForest(popularity ~ acousticness + danceability + duration_ms+ energy+ explicit+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence+ year+ key_1+ key_2+ key_3+ key_4+ key_5+ key_6+ key_7+ key_8+ key_9+ key_10+ key_11, data = predict_set.train)

stop.time<-proc.time() 
  
run.time<-stop.time -start.time 
  
print(run.time) 
  
stopCluster(cl) 
print(predict_set.rf_train)

```

Test model on predictions and get training errors
```{r}
predictions.rf = predict(predict_set.rf_train, newdata = predict_set.test)
paste("RMSE:",Metrics::rmse(predict_set.test$popularity, predictions.rf))
paste("MAE:",Metrics::mae(predict_set.test$popularity, predictions.rf))
paste("MASE:",Metrics::mase(predict_set.test$popularity, predictions.rf))

```
 Look at the importance of features of the trained random-forests model
```{r}
# Feature Explanations Plot
varImpPlot(predict_set.rf_train,main="Feature Importance of Random Forests Model")
```
##### Insights:
This random forests model has a relatively small error with an RMSE of 8.45 and an MAE of 6.58, but we recognize the following issues: \n

- A Low 47.53% of Variance Explained (usually, at least 60%+ is considered a good threshold) implies that there are other factors that influence music popularity, and these factors were not captured in the data. This may mean that the features in the dataset do not fully describe the factors that influence music popularity. \n

- From the features explanations plot, a large portion of the popularity prediction is dependent on year. This can be seen in the trend where the mean popularity of songs increases as the year increases. 



#### Train a Gradient Boosting Model to predict popularity


Prepare a different training set for this Gradient Boosting model
```{r}
# For this model, we split the popularity column away from the dataset and explicitly specify it as the y-variable.  
train_x = data.matrix(predict_set.train[, !names(predict_set.train) %in% c("popularity")])
train_y = predict_set.train$popularity

test_x = data.matrix(predict_set.test[, !names(predict_set.test) %in% c("popularity")])
test_y = predict_set.test$popularity


predict_set.xgb_train = xgb.DMatrix(data = train_x, label = train_y)
predict_set.xgb_test = xgb.DMatrix(data = test_x, label = test_y)

# Train an XGBoost model
# To avoid overfitting, we use relatively shallow trees (max-depth = 3), set early stopping rounds to 3, eta=0.10, and number of training rounds to 600
predict_set.xgb_model = xgboost(data = predict_set.xgb_train, max.depth = 3, nrounds = 600, early_stopping_rounds = 3,eta=0.10,nthread=3)
# Get model details
print(predict_set.xgb_model)
```

Test model on predictions and get training errors
```{r}
# Get predictions
predictions.xgb = predict(predict_set.xgb_model, predict_set.xgb_test)
paste("RMSE:",Metrics::rmse(predict_set.test$popularity, predictions.xgb))
paste("MAE:",Metrics::mae(predict_set.test$popularity, predictions.xgb))
paste("MASE:",Metrics::mase(predict_set.test$popularity, predictions.xgb))

```

We plot the original test set popularity against the predicted test set popularity
```{r}
x = 1:length(test_y)
plot(x, test_y, col = "red", type = "l")
lines(x, predictions.xgb, col = "blue", type = "l")
legend(x = 1, y = 38,  legend = c("Actual test set popularity", "Predicted test set popularity"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))
```

We compute the feature importance matrix for this model as well.
```{r}

# Compute feature importance matrix
names <- dimnames(data.matrix(predict_set.train[,-1]))[[2]]

importance_matrix <- xgb.importance(names, model = predict_set.xgb_model)
xgb.plot.importance(importance_matrix[1:10,])
```
##### Insights: 
Since fitting a random forest model was promising for popularity prediction, we try to fit a Gradient Boosting Model as a similar decision-tree based model, but with a potentially better fit. Gradient Boosting is a set of decision trees that are built in an additive (ensemble-like) manner, by introducing weak learners to improve the shortcomings of existing weak learners. Gradient boosting is also done in a sequential manner, by combining results along the way. However, this makes it easy to overfit a Gradient Boosting model, and we have done various tuning of the parameters to avoid overfitting. \n

- With RMSE at 8.45 and MAE at 6.61, this gradient boosting model was not able to improve upon the the previous random forests model. \n

- Looking at the popularity predictions against the test set popularity plot, it seems like the model does poorly at predicting the songs at the extremes of the popularity scale over time. \n

- Looking at the feature importance plot, the gradient boosting model also relies mostly on year for its prediction. Other than year, energy, mode, and duration_ms are significant features that are predictive of popularity. \n


<br />

#### Use AutomML to train and search for a model that accurately predicts popularity

We use H2O (an AutoML library) to automatically train models and optimize hyperparameters to optimize for predicting popularity.
```{r}
# Initialize H2O engine
h2o.init()

predict_set.h2o_train <- as.h2o(predict_set.train)
predict_set.h2o_test <- as.h2o(predict_set.test)
```

To reduce compute time, we train a maximum of 30 models. 
```{r}
h2o_train.aml <- h2o.automl(y = "popularity", training_frame = predict_set.h2o_train, max_models = 30)
lb <- h2o.get_leaderboard(h2o_train.aml)
# Print the model leaderboard (ranked by RMSE)
head(lb)
```
Use the best model (leader model) to predict on the test set.
```{r}
predictions <- h2o.predict(h2o_train.aml@leader, predict_set.h2o_test)
```
Get details about the most accurate model. 
```{r}
h2o_train.aml@leader
```


Take all models in the leaderboard, and use each model to predict on the test set. 
```{r}
perf_df<-data.frame(model_name=character(0),RMSE=numeric(0),MSE=numeric(0),MAE=numeric(0),RMSLE=numeric(0),mean_residual_deviance=numeric(0),r2=numeric(0))
mod_ids <- as_tibble(h2o_train.aml@leaderboard$model_id)
for(i in 1:6) {
  aml1 <- h2o.getModel(h2o_train.aml@leaderboard[i, 1]) # get model object in environment
  perf <- h2o.performance(aml1, predict_set.h2o_test)
  perf_df[nrow(perf_df) + 1,] = list(perf@metrics$model$name, perf@metrics$RMSE, perf@metrics$MSE, perf@metrics$mae, perf@metrics$rmsle, perf@metrics$mean_residual_deviance,  perf@metrics$r2)
 
  
}

# Print all performance metrics
perf_df

```
##### Insights:
After training up to 30 models using an AutoML library that searches for optimal model architectures and tunes model hyperparameters, the best leaderboard model was only able to slightly improve forecasting accuracy (RMSE at 8.44, MAE at 6.56). The best performing model was a complex stacked ensemble model comprised of 5 base models (1 deep learning model, 2 distributed random forests, 1 gradient boosting model, and 1 generalized linear model). Looking at the performance metric leaderboard on the test set, the low r2 of these models (47.67% for the most accurate model) implies that the given independent variables are still, in aggregate a poor predictor of popularity. In other words, these independent variables do not seem to capture the entire variance of popularity, and thus there may be other predictors of song popularity that are not captured within the dataset (i.e. frequency of the song's appearance in Twitter tweets). 

<br />


## **Performance Comparisons and Conclusions** 

##### Predicting Song Feature Trends: 
Using these values above for the predicted median value for the most popular songs, for each feature, music curators and record labels can predict which "sounds" will be most popular in the future. This information can then be used for future decision making with respect to which artists to sign and which sounds to push.


##### Predicting Song Popularity: 
Based on our chosen error metrics for model selection (RMSE, MAE, MASE), the best model that minimized these error metrics on the testing set was the Stacked Ensemble model trained via AutoML (RMSE: 8.44, MAE: 6.56). Our random forests model came at a close second with RMSE: 8.45, MAE: 6.58, followed by our gradient boosting model with RMSE: 8.54 and MAE: 6.61. Finally, our multiple linear regression model had RMSE: 8.98 and MAE: 6.98. \n

\n
The significance in the error metrics improvement from the linear model to the random forests model suggests that we were able to capture some significant non-linearities in the data. Nevertheless, despite the complexity of the Stacked Ensemble model, the improvement in prediction accuracy was minimal when compared to the random forests model. This suggests that the independent variables in the data do not fully capture the variance expressed in the dependent popularity variable. From the t-stat of the linear model and the feature importance plot of both the random forests and gradient boosting model, it was observed that year was by far the most significant factor used to explain popularity, which was seen as the average popularity of any song was highly correlated to the recency of the song's release date. Moreover, the low r2 in the leaderboard of the AutoML models and the 47% variance explained in the random forests model supports the fact that the independent variables used do not fully explain the popularity of a song. To further improve this prediction accuracy, we would suggest looking into alternative datasets (e.g. Twitter/Instagram mentions of a given song name) to supplement this dataset and potentially improve prediction accuracy. Finally, predicting song popularity to a very precise extent may be inherently difficult because of the various human influences/factors that dictates the popularity of any given song. Nonetheless, we have built a very strong foundation for predicting song popularity and have learned many lessons along the way.
\n
















