---
title: "Spotify Data Analysis and Descriptive Statistics"
output: html_notebook
author: CS Duals
date: November 18, 2020
---

1. Data Set Description
For this project, our team decided to take a data-driven approach to evaluate music. We are using the Spotify Dataset from 1921 to 2020 that consists of over 160,000 different tracks. In order to stay consistent in the evaluation, all data was sourced from the Spotify Web API. The following set of data is combination of Primary, Numerical, Dummy(binary), and Categorical data. This allows us to explore different types of models and draw different insights. Here are all the variables that is included in the data set:

Primary
    - id: this an unique key comprised of numbers and characters that is assigned to each track generated by Spotify

Numerical
    - acousticness: range from 0(LOW) to 1(HIGH)
    - danceability: range from 0(LOW) to 1(HIGH)
    - energy: range from 0(LOW) to 1(HIGH)
    - duration_ms: majority range from 200,000 to 300,000
    - instrumentalness: range from 0(LOW) to 1(HIGH)
    - valence: range from 0(LOW) to 1(HIGH)
    - popularity: range from 0(LOW) to 100(HIGH)*
    - tempo: majority range from 50(LOW) to 150(high)
    - liveness: range from 0(LOW) to 1(HIGH)
    - loudness majority range from -60 to 0
    - speechiness: range from 0(LOW) to 1(HIGH)
    - year: range from 1921 to 2020

Dummy
    - mode: 0 represents minor and 1 represents major
    - explicit: 0 represents no explicit content and 1 represents explicit content
    
Categorical
    - key: this consists of all different music keys on onctave encoded from 0 to 11 (i.e. C = 0, C# = 1, etc...)
    - artists: the artist of the track
    - release_date: the date of release in yyyy-mm-dd format
    - name: the name of the track

*With our approach on evaluating different tracks, we decided to use popularity as our main dependant variable. 


```{r}
options(repr.matrix.max.rows=100, repr.matrix.max.cols=20)
```


Import all Libraries
```{r}
# Libraries
options(warn=-1)
library(ggplot2)
library(dplyr)
library(plotly)
library(hrbrthemes)
library(forecast)
library(xts)
library(Metrics)
library(psych)
library(dygraphs)
library(GGally)
library(tidyverse)
library(tidyquant)  
library(cranlogs)   
library(corrr)      
library(cowplot)
library(readr)
library(plyr)
library(formattable)
library(wordcloud)
library(RWeka)
library(qdap)
library(tm)
library(dplyr)
library(magrittr)
library(corrplot)
library(treemap)
library(viridisLite)
library(factoextra)
library(gridExtra)
library(kableExtra)
```


```{r}
#Import Data into DF
df = read.csv(file='./data/data.csv')
df_genre = read.csv(file='./data/data_by_genres.csv')
df_artist = read.csv(file='./data/data_by_artist.csv')
df_year = read.csv(file='./data/data_by_year.csv')
df_genres2 = read.csv(file='./data/data_w_genres.csv')
```

2. Initial Analysis
Prior to any analysis, we needed to clean the data first to remove any invalid data. Next, we performed different statistical analysis such as correlation, network plot, normal distribution, time series graph, and many more to build a better understanding of our data set. These initial insights will allow us to create a robust model to perform different predictive analysis.   

```{r}
# Preview DF
head(df)
head(df_genre)
head(df_artist)
head(df_year)
head(df_genres2)
```

```{r}
# Find certain songs by feature and listen to them
df %>% arrange(desc(acousticness))
df %>% arrange(desc(danceability))
df %>% arrange(desc(valence))
```



Data Cleaning
```{r}
# Take only unique rows - remove duplicates
<<<<<<< HEAD
df <- distinct(df)

=======
df <- df %>% distinct()
>>>>>>> 0c1da7e44e0aa2f4defd37dcf0b0bb9683131bfc
# Remove null rows in R
df <- df[rowSums(is.na(df)) == 0,]
# Change year column to a date value
df$year <- as.Date(ISOdate(df$year, 1, 1))
```


Data Summary
```{r}
# Get summary of main dataframe
# Data Dimensions
dim(df)
# Statistical summary of every column
summary(df)
# Print all column names
colnames(df)
<<<<<<< HEAD


# Column names, column types, preview of data
str(df)
```

```{r}
top_artists <- df %>%
    group_by(artists) %>%
    dplyr::summarise(n_apperance = n()) 
top_artists
=======
>>>>>>> 0c1da7e44e0aa2f4defd37dcf0b0bb9683131bfc
```




Plot distributions of all values














Try to determine explanations for the top 10% of songs






Use K-Means to determine how many songs are similar


```{r}

```




Correlation Plot of numeric Features
```{r}
# Nice visualization of correlations
numerics = df[,!(colnames(df)  %in% c("id","year","artists","name","release_date"))]
tidyverse_static_correlations <- numerics %>% correlate() 
print.data.frame(tidyverse_static_correlations)
```

<<<<<<< HEAD


5-year rolling correlation of each numeric factor over time

```{r}
# Group by year and get mean of numeric features for each year
df_grped_year <- aggregate(df[,c(names(numerics),"year")], list(df$year), mean)
df_grped_year <- df_grped_year[,!names(df_grped_year) %in% "Group.1"]
df_grped_year
```
```{r}

static_corr_Year <-  df_grped_year[,!names(df_grped_year) %in% "year"] %>% correlate() 
print.data.frame(static_corr_Year)
=======
```{r}
# Correlation Plot
correlations = cor(numerics)
ggcorr(numerics, method = c("everything", "pearson")) 
```
```{r}
# Network plot
# To interpret this plot, variables that are more highly correlated appear closer together and are joined by stronger paths. Paths are colored by their
# sign (positive = blue, red = negative).
net_plot <- tidyverse_static_correlations %>%
    network_plot(colours = c("indianred2", "black", "skyblue1"),repel=TRUE, legend = TRUE) +
    labs(
        title = "Network Plot of Correlations of Numeric Features of Spotify Songs"
        ) +
    expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +
    theme_tq() +
    theme(legend.position = "right")
net_plot
>>>>>>> 0c1da7e44e0aa2f4defd37dcf0b0bb9683131bfc
```


```{r}
# Get rolling correlations
tidyverse_rolling_corr <- df_grped_year %>%
    # Data wrangling
    select(year, energy, popularity) %>%
    # Mutation
    tq_mutate_xy(
        x          = energy,
        y          = popularity,
        mutate_fun = runCor, 
        n          = 10,
        use        = "pairwise.complete.obs",
        col_rename = "rolling_corr"
    )
tidyverse_rolling_corr
```
```{r}
# Join static correlations with rolling correlations
static_corr_pop <- static_corr_Year %>% select(rowname, popularity) %>% rename(feature = rowname)
static_corr_pop
tidyverse_rolling_corr <- tidyverse_rolling_corr %>%
    left_join(static_corr_pop, by = "feature") %>%
    rename(static_corr = all_cran)
tidyverse_rolling_corr
```

```{r}
# Plot
tidyverse_rolling_corr %>%
    ggplot(aes(x = year, color="energy")) +
    # Data
    geom_point(aes(y = rolling_corr), alpha = 0.5) +
    # Aesthetics
    scale_color_tq() +
    labs(
        title = "Rolling Correlations between Popularity and Energy",
        x = "Year", y = "Correlation"
    ) +
    theme_tq() +
    theme(legend.position="none")

```


```{r}
#Time-Series Analysis
#duration_ts <- ts(df_year$duration_ms,start=df_year$year)
colnames(df_year)
subset_year <- df_year[,!(colnames(df_year)  %in% c("key", "mode", "duration_ms"))]
dygraph(subset_year, main = "Song Features Over Time") %>%
  dySeries("tempo", axis = 'y2', label='Tempo') %>%
  dySeries("popularity", axis = 'y2', label='Popularity') %>%
  dySeries("loudness", axis = 'y2', label='loudness') %>%
  dyCSS("legend.css")
```